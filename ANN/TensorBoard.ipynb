{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\KARAN S07\\\\Desktop\\\\Final Year\\\\Deep Learning\\\\TF-Course-Notebooks\\\\TF_Course_Notebooks\\\\DATA\\\\cancer_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1',axis=1).values\n",
    "y = df['benign_0__mal_1'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\",mode='min',verbose=1,patience=25)\n",
    "log_directory = 'logs\\\\fit'\n",
    "board = TensorBoard(log_dir=log_directory,histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# for binary classification the final layer should have the activation function as sigmoid since we need result \n",
    "# in the range of 0 and 1\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 4s 9ms/sample - loss: 0.6887 - val_loss: 0.6778\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 679us/sample - loss: 0.6794 - val_loss: 0.6683\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 571us/sample - loss: 0.6614 - val_loss: 0.6555\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 658us/sample - loss: 0.6643 - val_loss: 0.6416\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 639us/sample - loss: 0.6490 - val_loss: 0.6243\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 721us/sample - loss: 0.6225 - val_loss: 0.5987\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 700us/sample - loss: 0.6066 - val_loss: 0.5687\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 804us/sample - loss: 0.5913 - val_loss: 0.5376\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 816us/sample - loss: 0.5719 - val_loss: 0.5081\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 604us/sample - loss: 0.5479 - val_loss: 0.4826\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 588us/sample - loss: 0.5129 - val_loss: 0.4473\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 597us/sample - loss: 0.4766 - val_loss: 0.4120\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 608us/sample - loss: 0.4675 - val_loss: 0.3811\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 609us/sample - loss: 0.4394 - val_loss: 0.3554\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 594us/sample - loss: 0.4336 - val_loss: 0.3286\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 620us/sample - loss: 0.4077 - val_loss: 0.3045\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 585us/sample - loss: 0.3907 - val_loss: 0.2879\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 640us/sample - loss: 0.3724 - val_loss: 0.2715\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 621us/sample - loss: 0.3325 - val_loss: 0.2471\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 661us/sample - loss: 0.3318 - val_loss: 0.2298\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 613us/sample - loss: 0.3383 - val_loss: 0.2253\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 583us/sample - loss: 0.3377 - val_loss: 0.2159\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 689us/sample - loss: 0.3095 - val_loss: 0.2047\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 723us/sample - loss: 0.2941 - val_loss: 0.2030\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 590us/sample - loss: 0.2984 - val_loss: 0.1875\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 611us/sample - loss: 0.2789 - val_loss: 0.1801\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 579us/sample - loss: 0.2787 - val_loss: 0.1785\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 613us/sample - loss: 0.2815 - val_loss: 0.1722\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 694us/sample - loss: 0.2541 - val_loss: 0.1613\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 612us/sample - loss: 0.2820 - val_loss: 0.1594\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 610us/sample - loss: 0.2471 - val_loss: 0.1643\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 590us/sample - loss: 0.2565 - val_loss: 0.1531\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 592us/sample - loss: 0.2586 - val_loss: 0.1490\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 615us/sample - loss: 0.2204 - val_loss: 0.1508\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 613us/sample - loss: 0.2177 - val_loss: 0.1472\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 585us/sample - loss: 0.2255 - val_loss: 0.1354\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 615us/sample - loss: 0.2067 - val_loss: 0.1317\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 573us/sample - loss: 0.2131 - val_loss: 0.1360\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 656us/sample - loss: 0.1999 - val_loss: 0.1287\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 649us/sample - loss: 0.2159 - val_loss: 0.1242\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 648us/sample - loss: 0.2023 - val_loss: 0.1242\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 661us/sample - loss: 0.2092 - val_loss: 0.1274\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 632us/sample - loss: 0.1778 - val_loss: 0.1179\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 559us/sample - loss: 0.1982 - val_loss: 0.1154\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 632us/sample - loss: 0.2018 - val_loss: 0.1134\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 621us/sample - loss: 0.1825 - val_loss: 0.1218\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 606us/sample - loss: 0.1823 - val_loss: 0.1089\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 592us/sample - loss: 0.1788 - val_loss: 0.1099\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 592us/sample - loss: 0.1698 - val_loss: 0.1129\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 611us/sample - loss: 0.1798 - val_loss: 0.1060\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 631us/sample - loss: 0.1568 - val_loss: 0.1065\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 626us/sample - loss: 0.1592 - val_loss: 0.1132\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 585us/sample - loss: 0.1699 - val_loss: 0.1030\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 692us/sample - loss: 0.1619 - val_loss: 0.0992\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 604us/sample - loss: 0.1740 - val_loss: 0.1008\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 583us/sample - loss: 0.1358 - val_loss: 0.1021\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 662us/sample - loss: 0.1581 - val_loss: 0.1021\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 576us/sample - loss: 0.1523 - val_loss: 0.0968\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 604us/sample - loss: 0.1379 - val_loss: 0.0962\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 644us/sample - loss: 0.1501 - val_loss: 0.0962\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 604us/sample - loss: 0.1458 - val_loss: 0.0959\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 597us/sample - loss: 0.1383 - val_loss: 0.0925\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 594us/sample - loss: 0.1644 - val_loss: 0.0941\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 608us/sample - loss: 0.1430 - val_loss: 0.0930\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 606us/sample - loss: 0.1386 - val_loss: 0.0937\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 652us/sample - loss: 0.1399 - val_loss: 0.0943\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 665us/sample - loss: 0.1369 - val_loss: 0.0902\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 641us/sample - loss: 0.1362 - val_loss: 0.0995\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 667us/sample - loss: 0.1288 - val_loss: 0.0933\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 601us/sample - loss: 0.1297 - val_loss: 0.0893\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 590us/sample - loss: 0.1355 - val_loss: 0.0916\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 606us/sample - loss: 0.1461 - val_loss: 0.0888\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 557us/sample - loss: 0.1423 - val_loss: 0.1028\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 620us/sample - loss: 0.1339 - val_loss: 0.0893\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 604us/sample - loss: 0.1142 - val_loss: 0.0890\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 648us/sample - loss: 0.1537 - val_loss: 0.0957\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 576us/sample - loss: 0.1391 - val_loss: 0.0968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 597us/sample - loss: 0.1093 - val_loss: 0.0874\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 625us/sample - loss: 0.1174 - val_loss: 0.0902\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 604us/sample - loss: 0.1205 - val_loss: 0.1018\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 636us/sample - loss: 0.1141 - val_loss: 0.0930\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 606us/sample - loss: 0.1237 - val_loss: 0.0858\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 615us/sample - loss: 0.1170 - val_loss: 0.0872\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 630us/sample - loss: 0.1111 - val_loss: 0.0883\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 625us/sample - loss: 0.1392 - val_loss: 0.0884\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 647us/sample - loss: 0.1111 - val_loss: 0.0891\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 585us/sample - loss: 0.1309 - val_loss: 0.0949\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 627us/sample - loss: 0.1121 - val_loss: 0.0851\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 635us/sample - loss: 0.1255 - val_loss: 0.0912\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 630us/sample - loss: 0.1103 - val_loss: 0.0964\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 710us/sample - loss: 0.1314 - val_loss: 0.0882\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 571us/sample - loss: 0.1086 - val_loss: 0.1174\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 707us/sample - loss: 0.1091 - val_loss: 0.0894\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 647us/sample - loss: 0.1347 - val_loss: 0.0890\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 616us/sample - loss: 0.1097 - val_loss: 0.0914\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 624us/sample - loss: 0.1254 - val_loss: 0.0999\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 657us/sample - loss: 0.1208 - val_loss: 0.0881\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 614us/sample - loss: 0.1285 - val_loss: 0.0893\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 623us/sample - loss: 0.0933 - val_loss: 0.0924\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 680us/sample - loss: 0.1128 - val_loss: 0.0894\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 613us/sample - loss: 0.1233 - val_loss: 0.0852\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 708us/sample - loss: 0.1094 - val_loss: 0.0946\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 597us/sample - loss: 0.0995 - val_loss: 0.0937\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 781us/sample - loss: 0.1135 - val_loss: 0.0898\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 642us/sample - loss: 0.1089 - val_loss: 0.0827\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 637us/sample - loss: 0.1150 - val_loss: 0.1016\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 684us/sample - loss: 0.1149 - val_loss: 0.1031\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 597us/sample - loss: 0.1197 - val_loss: 0.0887\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 585us/sample - loss: 0.1133 - val_loss: 0.0829\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 623us/sample - loss: 0.1228 - val_loss: 0.0923\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 644us/sample - loss: 0.0984 - val_loss: 0.0976\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 680us/sample - loss: 0.1168 - val_loss: 0.0830\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 571us/sample - loss: 0.0985 - val_loss: 0.0895\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 592us/sample - loss: 0.1019 - val_loss: 0.0851\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 649us/sample - loss: 0.1161 - val_loss: 0.0957\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 657us/sample - loss: 0.0849 - val_loss: 0.1171\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 626us/sample - loss: 0.1182 - val_loss: 0.0984\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 627us/sample - loss: 0.1109 - val_loss: 0.0870\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 641us/sample - loss: 0.1117 - val_loss: 0.0924\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 654us/sample - loss: 0.0861 - val_loss: 0.1079\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 604us/sample - loss: 0.1024 - val_loss: 0.0982\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 734us/sample - loss: 0.0924 - val_loss: 0.0895\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 679us/sample - loss: 0.0869 - val_loss: 0.0834\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.091 - 0s 573us/sample - loss: 0.0916 - val_loss: 0.0855\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 729us/sample - loss: 0.1001 - val_loss: 0.0959\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 719us/sample - loss: 0.1145 - val_loss: 0.0838\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 639us/sample - loss: 0.0845 - val_loss: 0.0909\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 693us/sample - loss: 0.0954 - val_loss: 0.0901\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 729us/sample - loss: 0.0986 - val_loss: 0.0906\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 866us/sample - loss: 0.0904 - val_loss: 0.0937\n",
      "Epoch 00130: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb275620588>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test),\n",
    "         callbacks=[early_stop,board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
